{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# understanding of the data"
      ],
      "metadata": {
        "id": "b2ViGs1qd-la"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## basic imports\n"
      ],
      "metadata": {
        "id": "NffjR0h0Y54V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## importing important libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "izZp_piBY8on"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning\n",
        "### Handling Missing values\n",
        "1. Handling Missing values\n",
        "2. Handling Duplicates\n",
        "3. Check data type\n",
        "4. Understand the dataset"
      ],
      "metadata": {
        "id": "IkaEeIGnZDIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## getting to know the data"
      ],
      "metadata": {
        "id": "rUhDhuRxZgIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()\n",
        "df.describe()\n",
        "df.isnull().sum()\n",
        "df[<column_name>].value_counts()"
      ],
      "metadata": {
        "id": "wGFlTHKBY-13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove the duplicates\n",
        "\n",
        "```python\n",
        "df.drop_duplicates(inplace=True)\n",
        "```\n"
      ],
      "metadata": {
        "id": "GpwlAxdRexrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## getting the percentage of missing value"
      ],
      "metadata": {
        "id": "7Brgk0rwaYfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features_with_na=[features for features in df.columns if df[features].isnull().sum()>=1]\n",
        "for feature in features_with_na:\n",
        "    print(feature,np.round(df[feature].isnull().mean()*100,5), '% missing values')"
      ],
      "metadata": {
        "id": "oLmCiHPQac0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning column name"
      ],
      "metadata": {
        "id": "1w1BmT4hZVAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Gender'] = df['Gender'].replace('Fe Male', 'Female')\n"
      ],
      "metadata": {
        "id": "__WQo8CEZPWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## handiling the missing values"
      ],
      "metadata": {
        "id": "zyXzuUzRZ4-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```markdown\n",
        "# üß† Missing Value Imputation: Mean vs Median vs Mode\n",
        "\n",
        "When your dataset contains missing values, you often need to fill them\n",
        "(impute) to maintain data integrity.\n",
        "\n",
        "Choosing between **mean**, **median**, or **mode** depends on the **type** and **distribution** of your data.\n",
        "\n",
        "---\n",
        "\n",
        "## üßÆ Mean (Average)\n",
        "\n",
        "```python\n",
        "df['column'] = df['column'].fillna(df['column'].mean())\n",
        "```\n",
        "\n",
        "#### ‚úÖ When to use:\n",
        "- The column is **numerical**.\n",
        "- Data is **normally distributed** (i.e., symmetric).\n",
        "- No significant **outliers**.\n",
        "\n",
        "üìå **Example:** Heights, weights, temperatures.\n",
        "\n",
        "---\n",
        "\n",
        "### üìà Median (Middle value)\n",
        "\n",
        "```python\n",
        "df['column'] = df['column'].fillna(df['column'].median())\n",
        "```\n",
        "\n",
        "#### ‚úÖ When to use:\n",
        "- The column is **numerical**.\n",
        "- Data is **skewed** (i.e., has outliers).\n",
        "- You want a **robust** alternative to mean.\n",
        "\n",
        "üìå **Example:** Income, house prices, medical bills.\n",
        "\n",
        "---\n",
        "\n",
        "### üîÅ Mode (Most frequent value)\n",
        "\n",
        "```python\n",
        "df['column'] = df['column'].fillna(df['column'].mode()[0])\n",
        "```\n",
        "\n",
        "#### ‚úÖ When to use:\n",
        "- The column is **categorical** or contains **discrete values**.\n",
        "- You want to fill missing values with the **most frequent entry**.\n",
        "\n",
        "üìå **Example:** Gender, city, payment method.\n",
        "\n",
        "---\n",
        "\n",
        "### üìä Summary Table\n",
        "\n",
        "| Data Type       | Condition               | Recommended Method | Code Snippet                                         |\n",
        "|------------------|-------------------------|---------------------|------------------------------------------------------|\n",
        "| Numerical        | Symmetric, no outliers  | Mean                | `df['col'].fillna(df['col'].mean())`                |\n",
        "| Numerical        | Skewed, has outliers    | Median              | `df['col'].fillna(df['col'].median())`              |\n",
        "| Categorical      | Repeated common values  | Mode                | `df['col'].fillna(df['col'].mode()[0])`             |\n",
        "\n",
        "---\n",
        "\n",
        "üìù **Tip:** Always visualize and explore your data before choosing an imputation method. Use `.describe()` and `.hist()` or seaborn plots to understand the distribution.\n"
      ],
      "metadata": {
        "id": "xJrYnwQjbQEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç Feature Type Classification (Numerical, Categorical, Discrete, Continuous)\n"
      ],
      "metadata": {
        "id": "1sN1F0vkdU17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature type separation\n",
        "num_features = df.select_dtypes(exclude='object').columns.tolist()\n",
        "# num_features = [feature for feature in df.columns if df[feature].dtype != 'O']\n",
        "cat_features = df.select_dtypes(include='object').columns.tolist()\n",
        "discrete_features = [f for f in num_features if df[f].nunique() <= 25]\n",
        "continuous_features = [f for f in num_features if f not in discrete_features]\n",
        "\n",
        "# Print counts\n",
        "print(f'Numerical: {len(num_features)}, Categorical: {len(cat_features)}, Discrete: {len(discrete_features)}, Continuous: {len(continuous_features)}')\n"
      ],
      "metadata": {
        "id": "5275hdrIdViZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# making the dataset ready for machine learning"
      ],
      "metadata": {
        "id": "j6DCcRAQfNqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚öôÔ∏è Preprocessing: Encode Categoricals and Scale Numericals\n"
      ],
      "metadata": {
        "id": "zHoeecEHf5z-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " üîß `remainder='passthrough'` in `ColumnTransformer`\n",
        "\n",
        "- **What it does**: Keeps columns not explicitly transformed and **passes them through unchanged**.\n",
        "- **Is it compulsory?** No, default is `'drop'`, which removes untransformed columns.\n",
        "- **Use case**: Use `'passthrough'` if you want to retain those columns.\n",
        "\n",
        "**Example**:\n",
        "- `'passthrough'`: Keeps untransformed columns.\n",
        "- `'drop'`: Discards untransformed columns (default).\n"
      ],
      "metadata": {
        "id": "2fwVP_MzgpBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Column Transformer with 3 types of transformers\n",
        "cat_features = X.select_dtypes(include=\"object\").columns\n",
        "num_features = X.select_dtypes(exclude=\"object\").columns\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "numeric_transformer = StandardScaler()\n",
        "oh_transformer = OneHotEncoder(drop='first')\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    [\n",
        "         (\"OneHotEncoder\", oh_transformer, cat_features),\n",
        "          (\"StandardScaler\", numeric_transformer, num_features)\n",
        "    ],remainder='passthrough'\n",
        ")"
      ],
      "metadata": {
        "id": "aN4Myq-QgDIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the preprocessor on the training data\n",
        "X_train_transformed = preprocessor.fit_transform(X_train)\n",
        "# Fit the preprocessor on the training data\n",
        "X_train_transformed = preprocessor.fit_transform(X_train)\n"
      ],
      "metadata": {
        "id": "d8Y9AoeuhCH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ Train-Test Split for Model Evaluation\n"
      ],
      "metadata": {
        "id": "vHioInVIf1S7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = df.drop([<Target_col>], axis=1)\n",
        "y = df[<Target_col>]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "id": "sAiWpwphfWOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# machine learing"
      ],
      "metadata": {
        "id": "NOAojxtNidK9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç Analyzing Model Performance on Training and Test Data\n"
      ],
      "metadata": {
        "id": "D17j9zeBhsMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report,ConfusionMatrixDisplay, \\\n",
        "                            precision_score, recall_score, f1_score, roc_auc_score,roc_curve\n",
        "\n",
        "\n",
        "models={\n",
        "    \"Logisitic Regression\":LogisticRegression(),\n",
        "    \"Decision Tree\":DecisionTreeClassifier(),\n",
        "    \"Random Forest\":RandomForestClassifier(),\n",
        "    \"Gradient Boost\":GradientBoostingClassifier()\n",
        "}\n",
        "for i in range(len(list(models))):\n",
        "    model = list(models.values())[i]\n",
        "    model.fit(X_train, y_train) # Train model\n",
        "\n",
        "    # Make predictions\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    # Training set performance\n",
        "    model_train_accuracy = accuracy_score(y_train, y_train_pred) # Calculate Accuracy\n",
        "    model_train_f1 = f1_score(y_train, y_train_pred, average='weighted') # Calculate F1-score\n",
        "    model_train_precision = precision_score(y_train, y_train_pred) # Calculate Precision\n",
        "    model_train_recall = recall_score(y_train, y_train_pred) # Calculate Recall\n",
        "    model_train_rocauc_score = roc_auc_score(y_train, y_train_pred)\n",
        "\n",
        "\n",
        "    # Test set performance\n",
        "    model_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate Accuracy\n",
        "    model_test_f1 = f1_score(y_test, y_test_pred, average='weighted') # Calculate F1-score\n",
        "    model_test_precision = precision_score(y_test, y_test_pred) # Calculate Precision\n",
        "    model_test_recall = recall_score(y_test, y_test_pred) # Calculate Recall\n",
        "    model_test_rocauc_score = roc_auc_score(y_test, y_test_pred) #Calculate Roc\n",
        "\n",
        "\n",
        "    print(list(models.keys())[i])\n",
        "\n",
        "    print('Model performance for Training set')\n",
        "    print(\"- Accuracy: {:.4f}\".format(model_train_accuracy))\n",
        "    print('- F1 score: {:.4f}'.format(model_train_f1))\n",
        "\n",
        "    print('- Precision: {:.4f}'.format(model_train_precision))\n",
        "    print('- Recall: {:.4f}'.format(model_train_recall))\n",
        "    print('- Roc Auc Score: {:.4f}'.format(model_train_rocauc_score))\n",
        "\n",
        "\n",
        "\n",
        "    print('----------------------------------')\n",
        "\n",
        "    print('Model performance for Test set')\n",
        "    print('- Accuracy: {:.4f}'.format(model_test_accuracy))\n",
        "    print('- F1 score: {:.4f}'.format(model_test_f1))\n",
        "    print('- Precision: {:.4f}'.format(model_test_precision))\n",
        "    print('- Recall: {:.4f}'.format(model_test_recall))\n",
        "    print('- Roc Auc Score: {:.4f}'.format(model_test_rocauc_score))\n",
        "\n",
        "\n",
        "    print('='*35)\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "RK6YSFDIhs8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß Hyperparameter Tuning for Models"
      ],
      "metadata": {
        "id": "qyZC_vsKi2Em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize few parameter for Hyperparamter tuning\n",
        "knn_params = {\"n_neighbors\": [2, 3, 10, 20, 40, 50]}\n",
        "rf_params = {\"max_depth\": [5, 8, 15, None, 10],\n",
        "             \"max_features\": [5, 7, \"auto\", 8],\n",
        "             \"min_samples_split\": [2, 8, 15, 20],\n",
        "             \"n_estimators\": [100, 200, 500, 1000]}\n"
      ],
      "metadata": {
        "id": "-09Me8-3jCST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Models list for Hyperparameter tuning\n",
        "randomcv_models = [('KNN', KNeighborsRegressor(), knn_params),\n",
        "                   (\"RF\", RandomForestRegressor(), rf_params)\n",
        "\n",
        "                   ]"
      ],
      "metadata": {
        "id": "aYak2MfvjGkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Hyperparameter Tuning\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "model_param = {}\n",
        "for name, model, params in randomcv_models:\n",
        "    random = RandomizedSearchCV(estimator=model,\n",
        "                                   param_distributions=params,\n",
        "                                   n_iter=100,\n",
        "                                   cv=3,\n",
        "                                   verbose=2,\n",
        "                                   n_jobs=-1)\n",
        "    random.fit(X_train, y_train)\n",
        "    model_param[name] = random.best_params_\n",
        "\n",
        "for model_name in model_param:\n",
        "    print(f\"---------------- Best Params for {model_name} -------------------\")\n",
        "    print(model_param[model_name])"
      ],
      "metadata": {
        "id": "qMfOBIIkjJ_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### after getting the best model"
      ],
      "metadata": {
        "id": "2yr7SGcYjWW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Retraining the models with best parameters\n",
        "models = {\n",
        "    \"Random Forest Regressor\": RandomForestRegressor(n_estimators=100, min_samples_split=2, max_features='auto', max_depth=None,\n",
        "                                                     n_jobs=-1),\n",
        "     \"K-Neighbors Regressor\": KNeighborsRegressor(n_neighbors=10, n_jobs=-1)\n",
        "\n",
        "}\n",
        "for i in range(len(list(models))):\n",
        "    model = list(models.values())[i]\n",
        "    model.fit(X_train, y_train) # Train model\n",
        "\n",
        "    # Make predictions\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    model_train_mae , model_train_rmse, model_train_r2 = evaluate_model(y_train, y_train_pred)\n",
        "\n",
        "    model_test_mae , model_test_rmse, model_test_r2 = evaluate_model(y_test, y_test_pred)\n",
        "\n",
        "    print(list(models.keys())[i])\n",
        "\n",
        "    print('Model performance for Training set')\n",
        "    print(\"- Root Mean Squared Error: {:.4f}\".format(model_train_rmse))\n",
        "    print(\"- Mean Absolute Error: {:.4f}\".format(model_train_mae))\n",
        "    print(\"- R2 Score: {:.4f}\".format(model_train_r2))\n",
        "\n",
        "    print('----------------------------------')\n",
        "\n",
        "    print('Model performance for Test set')\n",
        "    print(\"- Root Mean Squared Error: {:.4f}\".format(model_test_rmse))\n",
        "    print(\"- Mean Absolute Error: {:.4f}\".format(model_test_mae))\n",
        "    print(\"- R2 Score: {:.4f}\".format(model_test_r2))\n",
        "\n",
        "    print('='*35)\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "LSyvIrG0je_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### roc curve"
      ],
      "metadata": {
        "id": "-M8EqJAfjlQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Plot ROC AUC Curve\n",
        "from sklearn.metrics import roc_auc_score,roc_curve\n",
        "plt.figure()\n",
        "\n",
        "# Add the models to the list that you want to view on the ROC plot\n",
        "auc_models = [\n",
        "{\n",
        "    'label': 'Random Forest Classifier',\n",
        "    'model': RandomForestClassifier(n_estimators=1000,min_samples_split=2,\n",
        "                                          max_features=7,max_depth=None),\n",
        "    'auc':  0.8325\n",
        "},\n",
        "\n",
        "]\n",
        "# create loop through all model\n",
        "for algo in auc_models:\n",
        "    model = algo['model'] # select the model\n",
        "    model.fit(X_train, y_train) # train the model\n",
        "# Compute False postive rate, and True positive rate\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:,1])\n",
        "# Calculate Area under the curve to display on the plot\n",
        "    plt.plot(fpr, tpr, label='%s ROC (area = %0.2f)' % (algo['label'], algo['auc']))\n",
        "# Custom settings for the plot\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('1-Specificity(False Positive Rate)')\n",
        "plt.ylabel('Sensitivity(True Positive Rate)')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig(\"auc.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fW0bm4cKjmjz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}